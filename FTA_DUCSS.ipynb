{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vh54VEzbh_ky",
        "4gktXqEih_k3",
        "6eDYqOTnh_k4",
        "cmAdf8Yih_k5"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shade-tree-software/passports/blob/main/FTA_DUCSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4arOf0uTh_kw"
      },
      "source": [
        "# Formula Trinity ü§ù DUCSS - Object Detection Workshop!\n",
        "\n",
        "Hello Everyone!\n",
        "\n",
        "Today we will be going through how to make a traffic cone detector using [PyTorch](https://pytorch.org/).\n",
        "\n",
        "This notebook was adapted from the one in [Fine-tuning Faster-RCNN using pytorch](https://www.kaggle.com/yerramvarun/fine-tuning-faster-rcnn-using-pytorch/notebook)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh54VEzbh_ky"
      },
      "source": [
        "## Installs and Imports\n",
        "\n",
        "Install and import PyTorch along with a few helper libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XkzKXmCh_kz"
      },
      "source": [
        "Let's install some dependencies and clone the [TorchVision Repo](https://github.com/pytorch/vision) so we can use some helper files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QAoM9pZbh_kz"
      },
      "source": [
        "# Install dependencies and \n",
        "!pip install albumentations==0.4.6\n",
        "!pip install pycocotools --quiet\n",
        "\n",
        "# Clone TorchVision repo and copy helper files\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "%cd vision\n",
        "!git checkout v0.3.0\n",
        "%cd ..\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bue8yKZOh_kz"
      },
      "source": [
        "Lets import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bT_SgoOBh_kz"
      },
      "source": [
        "# basic python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# helper libraries\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkwGcwwlh_k0"
      },
      "source": [
        "## The Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UXh4dpZh_k0"
      },
      "source": [
        "Having a good dataset is essential to training an accurate model. However, just possessing the dataset is not enough. We must pre-process it so that it can be fed to the neural network - and then understood by it.\n",
        "\n",
        "Finding an existing dataset can sometimes be very difficult. When searching for your dataset you may run into the following problems:\n",
        "\n",
        "* **Broken links**: even if someone has open-sourced something, links they previously had to their datasets often end up becoming stale.\n",
        "* **Poor Open-Source Practice**: some \"open-source\" dataset providers will insist on you contributing to the dataset to be provided access.\n",
        "\n",
        "However, keep searching! Here's some tricks:\n",
        "\n",
        "* **Search specific websites**: When searching on Google, you can search specifically on GitHub by searching something like: `cone detection dataset site:github.com`.\n",
        "* **Reading other people's code**: Try to find the code of someone else that worked on a similar problem, and see if you can find out what dataset they used! Such as searching: `image classification on traffic cones` and trying to find other notebooks.\n",
        "\n",
        "\n",
        "We found the dataset for this worksop by scouring GitHub repos. It contains 123 annotated images in the training set and \n",
        "\n",
        "Let's download it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHiRzz6otDoT"
      },
      "source": [
        "!wget -O data.rar https://www.dropbox.com/sh/ay9wf7ii81q5zif/AADwIb9HkvpBmUDJvKpNl0Xna?dl=0&file_subpath=%2Fimg&preview=data.rar#:~:text=Sign%20up-,Direct,-download\n",
        "!unrar x data.rar\n",
        "# image 96 had no annotations, so we'll delete it!\n",
        "!rm img/96.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "7K9qfPoTh_k1"
      },
      "source": [
        "# defining the files directory and testing directory\n",
        "files_dir = '/content/img/'\n",
        "test_dir = '/content/img1/'\n",
        "\n",
        "# we create a Dataset class which has a __getitem__ function and a __len__ function\n",
        "class ConeImagesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, files_dir, width, height, transforms=None):\n",
        "    self.transforms = transforms\n",
        "    self.files_dir = files_dir\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    \n",
        "    # sorting the images for consistency\n",
        "    # To get images, the extension of the filename is checked to be jpg\n",
        "    self.imgs = [image for image in sorted(os.listdir(files_dir)) if image[-4:]=='.jpg']\n",
        "    \n",
        "    # classes: 0 index is reserved for background\n",
        "    self.classes = [_, 'cone']\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = self.imgs[idx]\n",
        "    image_path = os.path.join(self.files_dir, img_name)\n",
        "\n",
        "    # reading the images and converting them to correct size and color    \n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "    # diving by 255\n",
        "    img_res /= 255.0\n",
        "    \n",
        "    # annotation file\n",
        "    annot_filename = img_name[:-4] + '.txt'\n",
        "    annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
        "    \n",
        "    boxes = []\n",
        "    labels = []\n",
        "    \n",
        "    # cv2 image gives size as height x width\n",
        "    wt = img.shape[1]\n",
        "    ht = img.shape[0]\n",
        "    \n",
        "    # box coordinates for xml files are extracted and corrected for image size given\n",
        "    with open(annot_file_path) as f:\n",
        "      for line in f:\n",
        "        labels.append(1)\n",
        "        \n",
        "        parsed = [float(x) for x in line.split(' ')]\n",
        "        x_center = parsed[1]\n",
        "        y_center = parsed[2]\n",
        "        box_wt = parsed[3]\n",
        "        box_ht = parsed[4]\n",
        "\n",
        "        xmin = x_center - box_wt/2\n",
        "        xmax = x_center + box_wt/2\n",
        "        ymin = y_center - box_ht/2\n",
        "        ymax = y_center + box_ht/2\n",
        "        \n",
        "        xmin_corr = int(xmin*self.width)\n",
        "        xmax_corr = int(xmax*self.width)\n",
        "        ymin_corr = int(ymin*self.height)\n",
        "        ymax_corr = int(ymax*self.height)\n",
        "        \n",
        "        boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "    \n",
        "    # convert boxes into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    \n",
        "    # getting the areas of the boxes\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "    \n",
        "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    image_id = torch.tensor([idx])\n",
        "    target[\"image_id\"] = image_id\n",
        "\n",
        "    if self.transforms:\n",
        "      sample = self.transforms(image = img_res,\n",
        "                                bboxes = target['boxes'],\n",
        "                                labels = labels)\n",
        "      img_res = sample['image']\n",
        "      target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "        \n",
        "    return img_res, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "\n",
        "# check dataset\n",
        "dataset = ConeImagesDataset(files_dir, 224, 224)\n",
        "print('Length of dataset:', len(dataset), '\\n')\n",
        "\n",
        "# getting the image and target for a test index.  Feel free to change the index.\n",
        "img, target = dataset[78]\n",
        "print('Image shape:', img.shape)\n",
        "print('Label example:', target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gktXqEih_k3"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "Let's make some a helper function to view our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WLZ-japJh_k4"
      },
      "source": [
        "# Function to visualize bounding boxes in the image\n",
        "def plot_img_bbox(img, target):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  fig, a = plt.subplots(1,1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  a.imshow(img)\n",
        "  for box in (target['boxes']):\n",
        "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'r',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    a.add_patch(rect)\n",
        "  plt.show()\n",
        "    \n",
        "# plotting the image with bboxes. Feel free to change the index\n",
        "img, target = dataset[25]\n",
        "plot_img_bbox(img, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eDYqOTnh_k4"
      },
      "source": [
        "# Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0p5LlZEh_k4"
      },
      "source": [
        "This is where we can apply augmentations to the image. \n",
        "\n",
        "The augmentations to object detection vary from normal augmentations becuase here we need to ensure that, bbox still aligns with the object correctly after transforming.\n",
        "\n",
        "Here we are doing a random flip transform.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nEy7h5mNh_k5"
      },
      "source": [
        "# Send train=True for training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return A.Compose(\n",
        "      [\n",
        "        A.HorizontalFlip(0.5),\n",
        "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "        ToTensorV2(p=1.0) \n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "  else:\n",
        "    return A.Compose(\n",
        "      [ToTensorV2(p=1.0)],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAdf8Yih_k5"
      },
      "source": [
        "# Dataloaders\n",
        "\n",
        "Make a loader for feeding our data into the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMfnPhczh_k5"
      },
      "source": [
        "Now lets prepare the datasets and dataloaders for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gxoYjbRUh_k5"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = ConeImagesDataset(files_dir, 480, 480, transforms=get_transform(train=True))\n",
        "dataset_test = ConeImagesDataset(files_dir, 480, 480, transforms=get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# train test split\n",
        "test_split = 0.2\n",
        "tsize = int(len(dataset)*test_split)\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "  dataset,\n",
        "  batch_size=10,\n",
        "  shuffle=True,\n",
        "  num_workers=4,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "  dataset_test,\n",
        "  batch_size=10,\n",
        "  shuffle=False,\n",
        "  num_workers=4,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HpLH-obh_k4"
      },
      "source": [
        "# Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PLGLZVbRh_k4"
      },
      "source": [
        "def get_object_detection_model(num_classes):\n",
        "  # load a model pre-trained pre-trained on COCO\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPsWQFoah_k5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-0h9b1Th_k5"
      },
      "source": [
        "Let's prepare the model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qP7T0cA-h_k5"
      },
      "source": [
        "# train on gpu if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 2 # one class (class 0) is dedicated to the \"background\"\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "  optimizer,\n",
        "  step_size=3,\n",
        "  gamma=0.1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq7wMpSQh_k5"
      },
      "source": [
        "Let the training begin!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "75qmxd-yh_k5"
      },
      "source": [
        "# training for 5 epochs\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training for one epoch\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmPxIhhHh_k5"
      },
      "source": [
        "# Filtering the outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaWlnf5hh_k5"
      },
      "source": [
        "Our model predicts a lot of bounding boxes per image, so take out the overlapping ones, we will use **Non Max Suppression** (NMS). If you want to brush up on that, check [this](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) out.\n",
        "\n",
        "Torchvision provides us a utility to apply NMS to our predictions, lets build a function `apply_nms` using that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5fsSRjrIh_k5"
      },
      "source": [
        "# the function takes the original prediction and the iou threshold.\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "  # torchvision returns the indices of the bboxes to keep\n",
        "  keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "  \n",
        "  final_prediction = orig_prediction\n",
        "  final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "  final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "  final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "  \n",
        "  return final_prediction\n",
        "\n",
        "# function to convert a torchtensor back to PIL image\n",
        "def torch_to_pil(img):\n",
        "  return torchtrans.ToPILImage()(img).convert('RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjAbCCp8h_k6"
      },
      "source": [
        "# Testing our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFV5aDbgh_k6"
      },
      "source": [
        "Now lets take an image from the test set and try to predict on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WOrNovPGh_k6"
      },
      "source": [
        "test_dataset = ConeImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n",
        "\n",
        "# pick one image from the test set\n",
        "img, target = test_dataset[10]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  prediction = model([img.to(device)])[0]\n",
        "    \n",
        "print('MODEL OUTPUT\\n')\n",
        "nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
        "\n",
        "plot_img_bbox(torch_to_pil(img), nms_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}